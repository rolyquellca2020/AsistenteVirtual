{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "chatbot_prueba.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhE-kRNUkuht",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b43e6b2b-1b53-4f2f-cf5a-57d9aa945b9b"
      },
      "source": [
        "#importando las librerias\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import numpy\n",
        "import tensorflow\n",
        "import random\n",
        "import json\n",
        "\n",
        "import pickle #guardar el modelo y no estar cargando a cada momento\n",
        "from nltk.stem.lancaster import LancasterStemmer #transformar palabras\n",
        "stemmer = LancasterStemmer()\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "04MUsJHOl600",
        "outputId": "a3b7d31e-b0f0-416f-d168-4febe5fe142f"
      },
      "source": [
        "#abriendo los datos e imprimiendo los datos\n",
        "with open(\"/content/db.json\", encoding='utf-8') as archivo:\n",
        "    datos = json.load(archivo)\n",
        "print(datos)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'contenido': [{'tag': 'saludo', 'patrones': ['hola', 'buenos dias', 'necesito ayuda'], 'respuestas': ['hola en que puedo ayudarle', 'buenos dias en que puedo ayudarle', 'Hola buenos dias, necesita ayuda']}, {'tag': 'examengeneral', 'patrones': ['cuando sera el examen general?', 'examen general', 'cuando es examen general'], 'respuestas': ['el examen general sera el 28 de setiembre.', 'el examen general sera el 28 de setiembre.', 'el examen general sera el 28 de setiembre.']}, {'tag': 'examencepre', 'patrones': ['cuando sera el examen cepreuna?', 'examen cepreuna', 'cuando es examen cepreuna'], 'respuestas': ['el examen cepreuna sera el 2 de octubre.mi estimado', 'el examen cepreuna sera el 2 de octubre.', 'el examen cepreuna sera el 2 de octubre. postulante']}, {'tag': 'carreras', 'patrones': ['cuantas carreras hay?', 'cuantas carreras?', 'cuantas carreras hay en la universidad'], 'respuestas': ['mi estimado(a) hay un total de 33 carreras', 'mi estimado(a) hay un total de 33 carreras en la UNA', 'mi estimado(a) hay un total de 33 carreras en la universidad']}, {'tag': 'rector', 'patrones': ['quien es el rector?', 'quien es el rector', 'quien es rector de la universidad'], 'respuestas': ['mi estimado(a) el rector actualmente es: Dr. Walter Zamalloa Cuba', 'mi estimado(a) el rector es: Dr. Walter Zamalloa Cuba', 'mi estimado(a) el rector de la universidad es: Dr. Walter Zamalloa Cuba']}, {'tag': 'inscripcionlugar', 'patrones': ['donde puedo inscribirme?', 'donde se inscribe?', 'donde se puede inscribirse?'], 'respuestas': ['Para obtener mas informacion ingrese al siguiente link: https://admision.unap.edu.pe/admision', 'mi estimado(a) para obtener mas informacion ingrese al siguiente link: https://admision.unap.edu.pe/admision', 'mi estimado(a) para obtener mas informacion ingrese al siguiente link: https://admision.unap.edu.pe/admision']}, {'tag': 'inscripcion', 'patrones': ['como puedo inscribirme?', 'como se podra inscribir?', 'como se puede inscribirse?'], 'respuestas': ['Para obtener mas informacion ingrese al siguiente link: https://admision.unap.edu.pe/admision', 'mi estimado(a) para obtener mas informacion ingrese al siguiente link: https://admision.unap.edu.pe/admision', 'mi estimado(a) para obtener mas informacion ingrese al siguiente link: https://admision.unap.edu.pe/admision']}, {'tag': 'despedida', 'patrones': ['adios', 'me retiro', 'me voy'], 'respuestas': ['adios mi estimado(a)', 'adios mi estimado(a)', 'adios mi estimado(a)']}]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2SEqQqot5TY"
      },
      "source": [
        "Separando las palabras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iF9-yVRkp8SV",
        "outputId": "adf0655b-9535-4de0-c4d2-ab3e0b416fb3"
      },
      "source": [
        "palabras = []\n",
        "tags = []\n",
        "datos_palabras = []\n",
        "datos_contenido = []\n",
        "for contenido in datos[\"contenido\"]:\n",
        "    for patrones in contenido[\"patrones\"]:\n",
        "        palabraux = nltk.word_tokenize(patrones) #toma una frase la separa el palabras\n",
        "        palabras.extend(palabraux)\n",
        "        datos_palabras.append(palabraux)\n",
        "        datos_contenido.append(contenido[\"tag\"])\n",
        "        if contenido[\"tag\"] not in tags:\n",
        "          tags.append(contenido[\"tag\"])\n",
        "print(palabras)\n",
        "print(datos_palabras)\n",
        "print(datos_contenido)\n",
        "print(tags) # palabras repetidas\n",
        "#nos ayudara a encontrar las  palabras clave"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['hola', 'buenos', 'dias', 'necesito', 'ayuda', 'cuando', 'sera', 'el', 'examen', 'general', '?', 'examen', 'general', 'cuando', 'es', 'examen', 'general', 'cuando', 'sera', 'el', 'examen', 'cepreuna', '?', 'examen', 'cepreuna', 'cuando', 'es', 'examen', 'cepreuna', 'cuantas', 'carreras', 'hay', '?', 'cuantas', 'carreras', '?', 'cuantas', 'carreras', 'hay', 'en', 'la', 'universidad', 'quien', 'es', 'el', 'rector', '?', 'quien', 'es', 'el', 'rector', 'quien', 'es', 'rector', 'de', 'la', 'universidad', 'donde', 'puedo', 'inscribirme', '?', 'donde', 'se', 'inscribe', '?', 'donde', 'se', 'puede', 'inscribirse', '?', 'como', 'puedo', 'inscribirme', '?', 'como', 'se', 'podra', 'inscribir', '?', 'como', 'se', 'puede', 'inscribirse', '?', 'adios', 'me', 'retiro', 'me', 'voy']\n",
            "[['hola'], ['buenos', 'dias'], ['necesito', 'ayuda'], ['cuando', 'sera', 'el', 'examen', 'general', '?'], ['examen', 'general'], ['cuando', 'es', 'examen', 'general'], ['cuando', 'sera', 'el', 'examen', 'cepreuna', '?'], ['examen', 'cepreuna'], ['cuando', 'es', 'examen', 'cepreuna'], ['cuantas', 'carreras', 'hay', '?'], ['cuantas', 'carreras', '?'], ['cuantas', 'carreras', 'hay', 'en', 'la', 'universidad'], ['quien', 'es', 'el', 'rector', '?'], ['quien', 'es', 'el', 'rector'], ['quien', 'es', 'rector', 'de', 'la', 'universidad'], ['donde', 'puedo', 'inscribirme', '?'], ['donde', 'se', 'inscribe', '?'], ['donde', 'se', 'puede', 'inscribirse', '?'], ['como', 'puedo', 'inscribirme', '?'], ['como', 'se', 'podra', 'inscribir', '?'], ['como', 'se', 'puede', 'inscribirse', '?'], ['adios'], ['me', 'retiro'], ['me', 'voy']]\n",
            "['saludo', 'saludo', 'saludo', 'examengeneral', 'examengeneral', 'examengeneral', 'examencepre', 'examencepre', 'examencepre', 'carreras', 'carreras', 'carreras', 'rector', 'rector', 'rector', 'inscripcionlugar', 'inscripcionlugar', 'inscripcionlugar', 'inscripcion', 'inscripcion', 'inscripcion', 'despedida', 'despedida', 'despedida']\n",
            "['saludo', 'examengeneral', 'examencepre', 'carreras', 'rector', 'inscripcionlugar', 'inscripcion', 'despedida']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKrsTUi9tnEQ",
        "outputId": "5aa6c2db-af91-408b-8cb7-81c7d24a5757"
      },
      "source": [
        "#pasamos palabras en minuscula recoremos toda la lista de palabras\n",
        "palabras = [stemmer.stem(w.lower()) for w in palabras if w!= \"?\" ]\n",
        "palabras = sorted(list(set(palabras)))\n",
        "tags = sorted(tags)\n",
        "\n",
        "entrenamiento = []\n",
        "salida = []\n",
        "salidaVacia = [0 for _ in range(len(tags))]\n",
        "#x = indice, documento = palabra\n",
        "for x, documento in enumerate(datos_palabras):\n",
        "    num=[]\n",
        "    palabraux = [stemmer.stem(w.lower()) for w in documento]\n",
        "    for w in palabras:\n",
        "       if w in palabraux:\n",
        "          num.append(1)\n",
        "       else:\n",
        "          num.append(0)\n",
        "    filaSalida = salidaVacia[:]\n",
        "    filaSalida[tags.index(datos_contenido[x])] = 1\n",
        "    entrenamiento.append(num)\n",
        "    salida.append(filaSalida)\n",
        "print(entrenamiento)\n",
        "print(salida)\n",
        "#1 palabra encontrada que se va a necesitar el entremanieto"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]]\n",
            "[[0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNyVUT1S0LQu"
      },
      "source": [
        "creando la red neuronal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hoTteMJj6pg6",
        "outputId": "d0fad707-1b47-402a-feb6-c29529c4c4d1"
      },
      "source": [
        "pip install tflearn"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tflearn\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e7/3c/0b156d08ef3d4e2a8009ecab2af1ad2e304f6fb99562b6271c68a74a4397/tflearn-0.5.0.tar.gz (107kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 4.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tflearn) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tflearn) (1.15.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from tflearn) (7.1.2)\n",
            "Building wheels for collected packages: tflearn\n",
            "  Building wheel for tflearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tflearn: filename=tflearn-0.5.0-cp37-none-any.whl size=127300 sha256=654f1c888366dd565e438c13b8a3f44277158a286a92a4f1bb0f7cea53a5f3fe\n",
            "  Stored in directory: /root/.cache/pip/wheels/31/d2/ed/fb9a0d301dd9586c11e9547120278e624227f22fd5f4baf744\n",
            "Successfully built tflearn\n",
            "Installing collected packages: tflearn\n",
            "Successfully installed tflearn-0.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bVr5_7XPzUgZ",
        "outputId": "b0e8c967-bbd3-4e39-b843-6c7c89159544"
      },
      "source": [
        "import tflearn\n",
        "from tensorflow.python.framework import ops\n",
        "ops.reset_default_graph()\n",
        "\n",
        "entrenamiento = numpy.array(entrenamiento)\n",
        "salida = numpy.array(salida)\n",
        "\n",
        "#tener espacio en blanco\n",
        "ops.reset_default_graph()\n",
        "\n",
        "#longitud de la una sola fila\n",
        "red = tflearn.input_data(shape=[None, len(entrenamiento[0])])\n",
        "red = tflearn.fully_connected(red, 24) #24 neuronas ocultas\n",
        "red = tflearn.fully_connected(red, 24) \n",
        "red = tflearn.fully_connected(red, len(salida[0]), activation=\"softmax\")\n",
        "red = tflearn.regression(red) #que tan efectivo es...\n",
        "\n",
        "\n",
        "modelo = tflearn.DNN(red)\n",
        "modelo.fit(entrenamiento, salida, n_epoch=400, batch_size=48, show_metric= True) #la cantidad de mismos datos , cantidad de entradas de datos\n",
        "modelo.save(\"modelo.tflearn\")\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "Run id: U5R9EX\n",
            "Log directory: /tmp/tflearn_logs/\n",
            "INFO:tensorflow:Summary name Accuracy/ (raw) is illegal; using Accuracy/__raw_ instead.\n",
            "---------------------------------\n",
            "Training samples: 24\n",
            "Validation samples: 0\n",
            "--\n",
            "Training Step: 1  | time: 0.081s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - acc: 0.0000 -- iter: 24/24\n",
            "--\n",
            "Training Step: 2  | total loss: \u001b[1m\u001b[32m1.87150\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 002 | loss: 1.87150 - acc: 0.0375 -- iter: 24/24\n",
            "--\n",
            "Training Step: 3  | total loss: \u001b[1m\u001b[32m2.04150\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 003 | loss: 2.04150 - acc: 0.2455 -- iter: 24/24\n",
            "--\n",
            "Training Step: 4  | total loss: \u001b[1m\u001b[32m2.06971\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 004 | loss: 2.06971 - acc: 0.4051 -- iter: 24/24\n",
            "--\n",
            "Training Step: 5  | total loss: \u001b[1m\u001b[32m2.07610\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 005 | loss: 2.07610 - acc: 0.2977 -- iter: 24/24\n",
            "--\n",
            "Training Step: 6  | total loss: \u001b[1m\u001b[32m2.07780\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 006 | loss: 2.07780 - acc: 0.2938 -- iter: 24/24\n",
            "--\n",
            "Training Step: 7  | total loss: \u001b[1m\u001b[32m2.07824\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 007 | loss: 2.07824 - acc: 0.4925 -- iter: 24/24\n",
            "--\n",
            "Training Step: 8  | total loss: \u001b[1m\u001b[32m2.07826\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 008 | loss: 2.07826 - acc: 0.6842 -- iter: 24/24\n",
            "--\n",
            "Training Step: 9  | total loss: \u001b[1m\u001b[32m2.07813\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 009 | loss: 2.07813 - acc: 0.7191 -- iter: 24/24\n",
            "--\n",
            "Training Step: 10  | total loss: \u001b[1m\u001b[32m2.07791\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 010 | loss: 2.07791 - acc: 0.7137 -- iter: 24/24\n",
            "--\n",
            "Training Step: 11  | total loss: \u001b[1m\u001b[32m2.07764\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 011 | loss: 2.07764 - acc: 0.7309 -- iter: 24/24\n",
            "--\n",
            "Training Step: 12  | total loss: \u001b[1m\u001b[32m2.07732\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 012 | loss: 2.07732 - acc: 0.7770 -- iter: 24/24\n",
            "--\n",
            "Training Step: 13  | total loss: \u001b[1m\u001b[32m2.07696\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 013 | loss: 2.07696 - acc: 0.8369 -- iter: 24/24\n",
            "--\n",
            "Training Step: 14  | total loss: \u001b[1m\u001b[32m2.07655\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 014 | loss: 2.07655 - acc: 0.8184 -- iter: 24/24\n",
            "--\n",
            "Training Step: 15  | total loss: \u001b[1m\u001b[32m2.07609\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 015 | loss: 2.07609 - acc: 0.8405 -- iter: 24/24\n",
            "--\n",
            "Training Step: 16  | total loss: \u001b[1m\u001b[32m2.07557\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 016 | loss: 2.07557 - acc: 0.8535 -- iter: 24/24\n",
            "--\n",
            "Training Step: 17  | total loss: \u001b[1m\u001b[32m2.07499\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 017 | loss: 2.07499 - acc: 0.8312 -- iter: 24/24\n",
            "--\n",
            "Training Step: 18  | total loss: \u001b[1m\u001b[32m2.07434\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 018 | loss: 2.07434 - acc: 0.8175 -- iter: 24/24\n",
            "--\n",
            "Training Step: 19  | total loss: \u001b[1m\u001b[32m2.07362\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 019 | loss: 2.07362 - acc: 0.8089 -- iter: 24/24\n",
            "--\n",
            "Training Step: 20  | total loss: \u001b[1m\u001b[32m2.07282\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 020 | loss: 2.07282 - acc: 0.8168 -- iter: 24/24\n",
            "--\n",
            "Training Step: 21  | total loss: \u001b[1m\u001b[32m2.07193\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 021 | loss: 2.07193 - acc: 0.8219 -- iter: 24/24\n",
            "--\n",
            "Training Step: 22  | total loss: \u001b[1m\u001b[32m2.07095\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 022 | loss: 2.07095 - acc: 0.8378 -- iter: 24/24\n",
            "--\n",
            "Training Step: 23  | total loss: \u001b[1m\u001b[32m2.06988\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 023 | loss: 2.06988 - acc: 0.8244 -- iter: 24/24\n",
            "--\n",
            "Training Step: 24  | total loss: \u001b[1m\u001b[32m2.06869\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 024 | loss: 2.06869 - acc: 0.8035 -- iter: 24/24\n",
            "--\n",
            "Training Step: 25  | total loss: \u001b[1m\u001b[32m2.06738\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 025 | loss: 2.06738 - acc: 0.7889 -- iter: 24/24\n",
            "--\n",
            "Training Step: 26  | total loss: \u001b[1m\u001b[32m2.06596\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 026 | loss: 2.06596 - acc: 0.7786 -- iter: 24/24\n",
            "--\n",
            "Training Step: 27  | total loss: \u001b[1m\u001b[32m2.06439\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 027 | loss: 2.06439 - acc: 0.7713 -- iter: 24/24\n",
            "--\n",
            "Training Step: 28  | total loss: \u001b[1m\u001b[32m2.06268\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 028 | loss: 2.06268 - acc: 0.7659 -- iter: 24/24\n",
            "--\n",
            "Training Step: 29  | total loss: \u001b[1m\u001b[32m2.06082\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 029 | loss: 2.06082 - acc: 0.7722 -- iter: 24/24\n",
            "--\n",
            "Training Step: 30  | total loss: \u001b[1m\u001b[32m2.05880\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 030 | loss: 2.05880 - acc: 0.7867 -- iter: 24/24\n",
            "--\n",
            "Training Step: 31  | total loss: \u001b[1m\u001b[32m2.05660\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 031 | loss: 2.05660 - acc: 0.7974 -- iter: 24/24\n",
            "--\n",
            "Training Step: 32  | total loss: \u001b[1m\u001b[32m2.05421\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 032 | loss: 2.05421 - acc: 0.8055 -- iter: 24/24\n",
            "--\n",
            "Training Step: 33  | total loss: \u001b[1m\u001b[32m2.05162\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 033 | loss: 2.05162 - acc: 0.8116 -- iter: 24/24\n",
            "--\n",
            "Training Step: 34  | total loss: \u001b[1m\u001b[32m2.04883\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 034 | loss: 2.04883 - acc: 0.8163 -- iter: 24/24\n",
            "--\n",
            "Training Step: 35  | total loss: \u001b[1m\u001b[32m2.04581\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 035 | loss: 2.04581 - acc: 0.8198 -- iter: 24/24\n",
            "--\n",
            "Training Step: 36  | total loss: \u001b[1m\u001b[32m2.04256\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 036 | loss: 2.04256 - acc: 0.8226 -- iter: 24/24\n",
            "--\n",
            "Training Step: 37  | total loss: \u001b[1m\u001b[32m2.03906\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 037 | loss: 2.03906 - acc: 0.8248 -- iter: 24/24\n",
            "--\n",
            "Training Step: 38  | total loss: \u001b[1m\u001b[32m2.03530\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 038 | loss: 2.03530 - acc: 0.8264 -- iter: 24/24\n",
            "--\n",
            "Training Step: 39  | total loss: \u001b[1m\u001b[32m2.03126\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 039 | loss: 2.03126 - acc: 0.8278 -- iter: 24/24\n",
            "--\n",
            "Training Step: 40  | total loss: \u001b[1m\u001b[32m2.02694\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 040 | loss: 2.02694 - acc: 0.8210 -- iter: 24/24\n",
            "--\n",
            "Training Step: 41  | total loss: \u001b[1m\u001b[32m2.02231\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 041 | loss: 2.02231 - acc: 0.8156 -- iter: 24/24\n",
            "--\n",
            "Training Step: 42  | total loss: \u001b[1m\u001b[32m2.01737\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 042 | loss: 2.01737 - acc: 0.8113 -- iter: 24/24\n",
            "--\n",
            "Training Step: 43  | total loss: \u001b[1m\u001b[32m2.01210\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 043 | loss: 2.01210 - acc: 0.8005 -- iter: 24/24\n",
            "--\n",
            "Training Step: 44  | total loss: \u001b[1m\u001b[32m2.00649\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 044 | loss: 2.00649 - acc: 0.7917 -- iter: 24/24\n",
            "--\n",
            "Training Step: 45  | total loss: \u001b[1m\u001b[32m2.00052\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 045 | loss: 2.00052 - acc: 0.7847 -- iter: 24/24\n",
            "--\n",
            "Training Step: 46  | total loss: \u001b[1m\u001b[32m1.99419\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 046 | loss: 1.99419 - acc: 0.7789 -- iter: 24/24\n",
            "--\n",
            "Training Step: 47  | total loss: \u001b[1m\u001b[32m1.98748\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 047 | loss: 1.98748 - acc: 0.7742 -- iter: 24/24\n",
            "--\n",
            "Training Step: 48  | total loss: \u001b[1m\u001b[32m1.97288\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 048 | loss: 1.97288 - acc: 0.7703 -- iter: 24/24\n",
            "--\n",
            "Training Step: 49  | total loss: \u001b[1m\u001b[32m1.97288\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 049 | loss: 1.97288 - acc: 0.7671 -- iter: 24/24\n",
            "--\n",
            "Training Step: 50  | total loss: \u001b[1m\u001b[32m1.96496\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 050 | loss: 1.96496 - acc: 0.7644 -- iter: 24/24\n",
            "--\n",
            "Training Step: 51  | total loss: \u001b[1m\u001b[32m1.95663\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 051 | loss: 1.95663 - acc: 0.7622 -- iter: 24/24\n",
            "--\n",
            "Training Step: 52  | total loss: \u001b[1m\u001b[32m1.94787\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 052 | loss: 1.94787 - acc: 0.7604 -- iter: 24/24\n",
            "--\n",
            "Training Step: 53  | total loss: \u001b[1m\u001b[32m1.93868\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 053 | loss: 1.93868 - acc: 0.7589 -- iter: 24/24\n",
            "--\n",
            "Training Step: 54  | total loss: \u001b[1m\u001b[32m1.92905\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 054 | loss: 1.92905 - acc: 0.7576 -- iter: 24/24\n",
            "--\n",
            "Training Step: 55  | total loss: \u001b[1m\u001b[32m1.91898\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 055 | loss: 1.91898 - acc: 0.7565 -- iter: 24/24\n",
            "--\n",
            "Training Step: 56  | total loss: \u001b[1m\u001b[32m1.90848\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 056 | loss: 1.90848 - acc: 0.7556 -- iter: 24/24\n",
            "--\n",
            "Training Step: 57  | total loss: \u001b[1m\u001b[32m1.89753\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 057 | loss: 1.89753 - acc: 0.7548 -- iter: 24/24\n",
            "--\n",
            "Training Step: 58  | total loss: \u001b[1m\u001b[32m1.88614\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 058 | loss: 1.88614 - acc: 0.7541 -- iter: 24/24\n",
            "--\n",
            "Training Step: 59  | total loss: \u001b[1m\u001b[32m1.87433\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 059 | loss: 1.87433 - acc: 0.7592 -- iter: 24/24\n",
            "--\n",
            "Training Step: 60  | total loss: \u001b[1m\u001b[32m1.86209\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 060 | loss: 1.86209 - acc: 0.7635 -- iter: 24/24\n",
            "--\n",
            "Training Step: 61  | total loss: \u001b[1m\u001b[32m1.84943\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 061 | loss: 1.84943 - acc: 0.7672 -- iter: 24/24\n",
            "--\n",
            "Training Step: 62  | total loss: \u001b[1m\u001b[32m1.83637\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 062 | loss: 1.83637 - acc: 0.7703 -- iter: 24/24\n",
            "--\n",
            "Training Step: 63  | total loss: \u001b[1m\u001b[32m1.82291\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 063 | loss: 1.82291 - acc: 0.7730 -- iter: 24/24\n",
            "--\n",
            "Training Step: 64  | total loss: \u001b[1m\u001b[32m1.80909\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 064 | loss: 1.80909 - acc: 0.7754 -- iter: 24/24\n",
            "--\n",
            "Training Step: 65  | total loss: \u001b[1m\u001b[32m1.79490\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 065 | loss: 1.79490 - acc: 0.7774 -- iter: 24/24\n",
            "--\n",
            "Training Step: 66  | total loss: \u001b[1m\u001b[32m1.78038\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 066 | loss: 1.78038 - acc: 0.7791 -- iter: 24/24\n",
            "--\n",
            "Training Step: 67  | total loss: \u001b[1m\u001b[32m1.76554\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 067 | loss: 1.76554 - acc: 0.7806 -- iter: 24/24\n",
            "--\n",
            "Training Step: 68  | total loss: \u001b[1m\u001b[32m1.75040\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 068 | loss: 1.75040 - acc: 0.7819 -- iter: 24/24\n",
            "--\n",
            "Training Step: 69  | total loss: \u001b[1m\u001b[32m1.73499\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 069 | loss: 1.73499 - acc: 0.7841 -- iter: 24/24\n",
            "--\n",
            "Training Step: 70  | total loss: \u001b[1m\u001b[32m1.71934\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 070 | loss: 1.71934 - acc: 0.7841 -- iter: 24/24\n",
            "--\n",
            "Training Step: 71  | total loss: \u001b[1m\u001b[32m1.68738\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 071 | loss: 1.68738 - acc: 0.7857 -- iter: 24/24\n",
            "--\n",
            "Training Step: 72  | total loss: \u001b[1m\u001b[32m1.68738\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 072 | loss: 1.68738 - acc: 0.7857 -- iter: 24/24\n",
            "--\n",
            "Training Step: 73  | total loss: \u001b[1m\u001b[32m1.67114\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 073 | loss: 1.67114 - acc: 0.7863 -- iter: 24/24\n",
            "--\n",
            "Training Step: 74  | total loss: \u001b[1m\u001b[32m1.65475\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 074 | loss: 1.65475 - acc: 0.7869 -- iter: 24/24\n",
            "--\n",
            "Training Step: 75  | total loss: \u001b[1m\u001b[32m1.63825\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 075 | loss: 1.63825 - acc: 0.7829 -- iter: 24/24\n",
            "--\n",
            "Training Step: 76  | total loss: \u001b[1m\u001b[32m1.62165\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 076 | loss: 1.62165 - acc: 0.7763 -- iter: 24/24\n",
            "--\n",
            "Training Step: 77  | total loss: \u001b[1m\u001b[32m1.60498\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 077 | loss: 1.60498 - acc: 0.7763 -- iter: 24/24\n",
            "--\n",
            "Training Step: 78  | total loss: \u001b[1m\u001b[32m1.58827\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 078 | loss: 1.58827 - acc: 0.7692 -- iter: 24/24\n",
            "--\n",
            "Training Step: 79  | total loss: \u001b[1m\u001b[32m1.57153\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 079 | loss: 1.57153 - acc: 0.7629 -- iter: 24/24\n",
            "--\n",
            "Training Step: 80  | total loss: \u001b[1m\u001b[32m1.55480\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 080 | loss: 1.55480 - acc: 0.7530 -- iter: 24/24\n",
            "--\n",
            "Training Step: 81  | total loss: \u001b[1m\u001b[32m1.53808\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 081 | loss: 1.53808 - acc: 0.7443 -- iter: 24/24\n",
            "--\n",
            "Training Step: 82  | total loss: \u001b[1m\u001b[32m1.52140\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 082 | loss: 1.52140 - acc: 0.7365 -- iter: 24/24\n",
            "--\n",
            "Training Step: 83  | total loss: \u001b[1m\u001b[32m1.50459\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 083 | loss: 1.50459 - acc: 0.7296 -- iter: 24/24\n",
            "--\n",
            "Training Step: 84  | total loss: \u001b[1m\u001b[32m1.48769\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 084 | loss: 1.48769 - acc: 0.7233 -- iter: 24/24\n",
            "--\n",
            "Training Step: 85  | total loss: \u001b[1m\u001b[32m1.47071\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 085 | loss: 1.47071 - acc: 0.7176 -- iter: 24/24\n",
            "--\n",
            "Training Step: 86  | total loss: \u001b[1m\u001b[32m1.45371\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 086 | loss: 1.45371 - acc: 0.7083 -- iter: 24/24\n",
            "--\n",
            "Training Step: 87  | total loss: \u001b[1m\u001b[32m1.43669\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 087 | loss: 1.43669 - acc: 0.7000 -- iter: 24/24\n",
            "--\n",
            "Training Step: 88  | total loss: \u001b[1m\u001b[32m1.41968\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 088 | loss: 1.41968 - acc: 0.6925 -- iter: 24/24\n",
            "--\n",
            "Training Step: 89  | total loss: \u001b[1m\u001b[32m1.40270\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 089 | loss: 1.40270 - acc: 0.6816 -- iter: 24/24\n",
            "--\n",
            "Training Step: 90  | total loss: \u001b[1m\u001b[32m1.38578\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 090 | loss: 1.38578 - acc: 0.6718 -- iter: 24/24\n",
            "--\n",
            "Training Step: 91  | total loss: \u001b[1m\u001b[32m1.36891\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 091 | loss: 1.36891 - acc: 0.6629 -- iter: 24/24\n",
            "--\n",
            "Training Step: 92  | total loss: \u001b[1m\u001b[32m1.35212\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 092 | loss: 1.35212 - acc: 0.6550 -- iter: 24/24\n",
            "--\n",
            "Training Step: 93  | total loss: \u001b[1m\u001b[32m1.33542\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 093 | loss: 1.33542 - acc: 0.6478 -- iter: 24/24\n",
            "--\n",
            "Training Step: 94  | total loss: \u001b[1m\u001b[32m1.50034\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 094 | loss: 1.50034 - acc: 0.5955 -- iter: 24/24\n",
            "--\n",
            "Training Step: 95  | total loss: \u001b[1m\u001b[32m1.46622\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 095 | loss: 1.46622 - acc: 0.5985 -- iter: 24/24\n",
            "--\n",
            "Training Step: 96  | total loss: \u001b[1m\u001b[32m1.60484\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 096 | loss: 1.60484 - acc: 0.5470 -- iter: 24/24\n",
            "--\n",
            "Training Step: 97  | total loss: \u001b[1m\u001b[32m1.55853\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 097 | loss: 1.55853 - acc: 0.5506 -- iter: 24/24\n",
            "--\n",
            "Training Step: 98  | total loss: \u001b[1m\u001b[32m1.51610\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 098 | loss: 1.51610 - acc: 0.5539 -- iter: 24/24\n",
            "--\n",
            "Training Step: 99  | total loss: \u001b[1m\u001b[32m1.47712\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 099 | loss: 1.47712 - acc: 0.5568 -- iter: 24/24\n",
            "--\n",
            "Training Step: 100  | total loss: \u001b[1m\u001b[32m1.62852\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 100 | loss: 1.62852 - acc: 0.5178 -- iter: 24/24\n",
            "--\n",
            "Training Step: 101  | total loss: \u001b[1m\u001b[32m1.57694\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 101 | loss: 1.57694 - acc: 0.5244 -- iter: 24/24\n",
            "--\n",
            "Training Step: 102  | total loss: \u001b[1m\u001b[32m1.52994\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 102 | loss: 1.52994 - acc: 0.5303 -- iter: 24/24\n",
            "--\n",
            "Training Step: 103  | total loss: \u001b[1m\u001b[32m1.48700\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 103 | loss: 1.48700 - acc: 0.5356 -- iter: 24/24\n",
            "--\n",
            "Training Step: 104  | total loss: \u001b[1m\u001b[32m1.44766\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 104 | loss: 1.44766 - acc: 0.5403 -- iter: 24/24\n",
            "--\n",
            "Training Step: 105  | total loss: \u001b[1m\u001b[32m1.41152\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 105 | loss: 1.41152 - acc: 0.5446 -- iter: 24/24\n",
            "--\n",
            "Training Step: 106  | total loss: \u001b[1m\u001b[32m1.37820\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 106 | loss: 1.37820 - acc: 0.5485 -- iter: 24/24\n",
            "--\n",
            "Training Step: 107  | total loss: \u001b[1m\u001b[32m1.34739\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 107 | loss: 1.34739 - acc: 0.5562 -- iter: 24/24\n",
            "--\n",
            "Training Step: 108  | total loss: \u001b[1m\u001b[32m1.31880\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 108 | loss: 1.31880 - acc: 0.5672 -- iter: 24/24\n",
            "--\n",
            "Training Step: 109  | total loss: \u001b[1m\u001b[32m1.29218\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 109 | loss: 1.29218 - acc: 0.5813 -- iter: 24/24\n",
            "--\n",
            "Training Step: 110  | total loss: \u001b[1m\u001b[32m1.26730\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 110 | loss: 1.26730 - acc: 0.5940 -- iter: 24/24\n",
            "--\n",
            "Training Step: 111  | total loss: \u001b[1m\u001b[32m1.24396\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 111 | loss: 1.24396 - acc: 0.6055 -- iter: 24/24\n",
            "--\n",
            "Training Step: 112  | total loss: \u001b[1m\u001b[32m1.22200\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 112 | loss: 1.22200 - acc: 0.6157 -- iter: 24/24\n",
            "--\n",
            "Training Step: 113  | total loss: \u001b[1m\u001b[32m1.20125\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 113 | loss: 1.20125 - acc: 0.6250 -- iter: 24/24\n",
            "--\n",
            "Training Step: 114  | total loss: \u001b[1m\u001b[32m1.38681\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 114 | loss: 1.38681 - acc: 0.5708 -- iter: 24/24\n",
            "--\n",
            "Training Step: 115  | total loss: \u001b[1m\u001b[32m1.34798\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 115 | loss: 1.34798 - acc: 0.5804 -- iter: 24/24\n",
            "--\n",
            "Training Step: 116  | total loss: \u001b[1m\u001b[32m1.31239\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 116 | loss: 1.31239 - acc: 0.5890 -- iter: 24/24\n",
            "--\n",
            "Training Step: 117  | total loss: \u001b[1m\u001b[32m1.27967\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 117 | loss: 1.27967 - acc: 0.5968 -- iter: 24/24\n",
            "--\n",
            "Training Step: 118  | total loss: \u001b[1m\u001b[32m1.24949\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 118 | loss: 1.24949 - acc: 0.6038 -- iter: 24/24\n",
            "--\n",
            "Training Step: 119  | total loss: \u001b[1m\u001b[32m1.22158\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 119 | loss: 1.22158 - acc: 0.6101 -- iter: 24/24\n",
            "--\n",
            "Training Step: 120  | total loss: \u001b[1m\u001b[32m1.19568\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 120 | loss: 1.19568 - acc: 0.6157 -- iter: 24/24\n",
            "--\n",
            "Training Step: 121  | total loss: \u001b[1m\u001b[32m1.17156\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 121 | loss: 1.17156 - acc: 0.6208 -- iter: 24/24\n",
            "--\n",
            "Training Step: 122  | total loss: \u001b[1m\u001b[32m1.14904\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 122 | loss: 1.14904 - acc: 0.6254 -- iter: 24/24\n",
            "--\n",
            "Training Step: 123  | total loss: \u001b[1m\u001b[32m1.12793\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 123 | loss: 1.12793 - acc: 0.6295 -- iter: 24/24\n",
            "--\n",
            "Training Step: 124  | total loss: \u001b[1m\u001b[32m1.32610\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 124 | loss: 1.32610 - acc: 0.5874 -- iter: 24/24\n",
            "--\n",
            "Training Step: 125  | total loss: \u001b[1m\u001b[32m1.28592\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 125 | loss: 1.28592 - acc: 0.5995 -- iter: 24/24\n",
            "--\n",
            "Training Step: 126  | total loss: \u001b[1m\u001b[32m1.24921\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 126 | loss: 1.24921 - acc: 0.6146 -- iter: 24/24\n",
            "--\n",
            "Training Step: 127  | total loss: \u001b[1m\u001b[32m1.21558\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 127 | loss: 1.21558 - acc: 0.6281 -- iter: 24/24\n",
            "--\n",
            "Training Step: 128  | total loss: \u001b[1m\u001b[32m1.18469\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 128 | loss: 1.18469 - acc: 0.6445 -- iter: 24/24\n",
            "--\n",
            "Training Step: 129  | total loss: \u001b[1m\u001b[32m1.15624\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 129 | loss: 1.15624 - acc: 0.6592 -- iter: 24/24\n",
            "--\n",
            "Training Step: 130  | total loss: \u001b[1m\u001b[32m1.42167\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 130 | loss: 1.42167 - acc: 0.6016 -- iter: 24/24\n",
            "--\n",
            "Training Step: 131  | total loss: \u001b[1m\u001b[32m1.36860\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 131 | loss: 1.36860 - acc: 0.6206 -- iter: 24/24\n",
            "--\n",
            "Training Step: 132  | total loss: \u001b[1m\u001b[32m1.32052\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 132 | loss: 1.32052 - acc: 0.6377 -- iter: 24/24\n",
            "--\n",
            "Training Step: 133  | total loss: \u001b[1m\u001b[32m1.27689\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 133 | loss: 1.27689 - acc: 0.6573 -- iter: 24/24\n",
            "--\n",
            "Training Step: 134  | total loss: \u001b[1m\u001b[32m1.42403\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 134 | loss: 1.42403 - acc: 0.6386 -- iter: 24/24\n",
            "--\n",
            "Training Step: 135  | total loss: \u001b[1m\u001b[32m1.36947\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 135 | loss: 1.36947 - acc: 0.6386 -- iter: 24/24\n",
            "--\n",
            "Training Step: 136  | total loss: \u001b[1m\u001b[32m1.32014\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 136 | loss: 1.32014 - acc: 0.6623 -- iter: 24/24\n",
            "--\n",
            "Training Step: 137  | total loss: \u001b[1m\u001b[32m1.27546\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 137 | loss: 1.27546 - acc: 0.6835 -- iter: 24/24\n",
            "--\n",
            "Training Step: 138  | total loss: \u001b[1m\u001b[32m1.48045\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 138 | loss: 1.48045 - acc: 0.6235 -- iter: 24/24\n",
            "--\n",
            "Training Step: 139  | total loss: \u001b[1m\u001b[32m1.41934\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 139 | loss: 1.41934 - acc: 0.6487 -- iter: 24/24\n",
            "--\n",
            "Training Step: 140  | total loss: \u001b[1m\u001b[32m1.36420\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 140 | loss: 1.36420 - acc: 0.6713 -- iter: 24/24\n",
            "--\n",
            "Training Step: 141  | total loss: \u001b[1m\u001b[32m1.31436\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 141 | loss: 1.31436 - acc: 0.6958 -- iter: 24/24\n",
            "--\n",
            "Training Step: 142  | total loss: \u001b[1m\u001b[32m1.26923\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 142 | loss: 1.26923 - acc: 0.7179 -- iter: 24/24\n",
            "--\n",
            "Training Step: 143  | total loss: \u001b[1m\u001b[32m1.22827\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 143 | loss: 1.22827 - acc: 0.7378 -- iter: 24/24\n",
            "--\n",
            "Training Step: 144  | total loss: \u001b[1m\u001b[32m1.19100\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 144 | loss: 1.19100 - acc: 0.7557 -- iter: 24/24\n",
            "--\n",
            "Training Step: 145  | total loss: \u001b[1m\u001b[32m1.15701\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 145 | loss: 1.15701 - acc: 0.7718 -- iter: 24/24\n",
            "--\n",
            "Training Step: 146  | total loss: \u001b[1m\u001b[32m1.12592\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 146 | loss: 1.12592 - acc: 0.7863 -- iter: 24/24\n",
            "--\n",
            "Training Step: 147  | total loss: \u001b[1m\u001b[32m1.09740\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 147 | loss: 1.09740 - acc: 0.7993 -- iter: 24/24\n",
            "--\n",
            "Training Step: 148  | total loss: \u001b[1m\u001b[32m1.07115\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 148 | loss: 1.07115 - acc: 0.8110 -- iter: 24/24\n",
            "--\n",
            "Training Step: 149  | total loss: \u001b[1m\u001b[32m1.04692\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 149 | loss: 1.04692 - acc: 0.8216 -- iter: 24/24\n",
            "--\n",
            "Training Step: 150  | total loss: \u001b[1m\u001b[32m1.22931\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 150 | loss: 1.22931 - acc: 0.7478 -- iter: 24/24\n",
            "--\n",
            "Training Step: 151  | total loss: \u001b[1m\u001b[32m1.18826\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 151 | loss: 1.18826 - acc: 0.7647 -- iter: 24/24\n",
            "--\n",
            "Training Step: 152  | total loss: \u001b[1m\u001b[32m1.15090\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 152 | loss: 1.15090 - acc: 0.7799 -- iter: 24/24\n",
            "--\n",
            "Training Step: 153  | total loss: \u001b[1m\u001b[32m1.11682\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 153 | loss: 1.11682 - acc: 0.7935 -- iter: 24/24\n",
            "--\n",
            "Training Step: 154  | total loss: \u001b[1m\u001b[32m1.08564\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 154 | loss: 1.08564 - acc: 0.8059 -- iter: 24/24\n",
            "--\n",
            "Training Step: 155  | total loss: \u001b[1m\u001b[32m1.05704\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 155 | loss: 1.05704 - acc: 0.8169 -- iter: 24/24\n",
            "--\n",
            "Training Step: 156  | total loss: \u001b[1m\u001b[32m1.03074\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 156 | loss: 1.03074 - acc: 0.8269 -- iter: 24/24\n",
            "--\n",
            "Training Step: 157  | total loss: \u001b[1m\u001b[32m1.00648\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 157 | loss: 1.00648 - acc: 0.8359 -- iter: 24/24\n",
            "--\n",
            "Training Step: 158  | total loss: \u001b[1m\u001b[32m0.98402\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 158 | loss: 0.98402 - acc: 0.8440 -- iter: 24/24\n",
            "--\n",
            "Training Step: 159  | total loss: \u001b[1m\u001b[32m0.96317\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 159 | loss: 0.96317 - acc: 0.8512 -- iter: 24/24\n",
            "--\n",
            "Training Step: 160  | total loss: \u001b[1m\u001b[32m0.94375\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 160 | loss: 0.94375 - acc: 0.8578 -- iter: 24/24\n",
            "--\n",
            "Training Step: 161  | total loss: \u001b[1m\u001b[32m0.92559\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 161 | loss: 0.92559 - acc: 0.8637 -- iter: 24/24\n",
            "--\n",
            "Training Step: 162  | total loss: \u001b[1m\u001b[32m0.90857\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 162 | loss: 0.90857 - acc: 0.8690 -- iter: 24/24\n",
            "--\n",
            "Training Step: 163  | total loss: \u001b[1m\u001b[32m0.89254\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 163 | loss: 0.89254 - acc: 0.8737 -- iter: 24/24\n",
            "--\n",
            "Training Step: 164  | total loss: \u001b[1m\u001b[32m0.87742\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 164 | loss: 0.87742 - acc: 0.8780 -- iter: 24/24\n",
            "--\n",
            "Training Step: 165  | total loss: \u001b[1m\u001b[32m0.86308\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 165 | loss: 0.86308 - acc: 0.8819 -- iter: 24/24\n",
            "--\n",
            "Training Step: 166  | total loss: \u001b[1m\u001b[32m0.84946\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 166 | loss: 0.84946 - acc: 0.8854 -- iter: 24/24\n",
            "--\n",
            "Training Step: 167  | total loss: \u001b[1m\u001b[32m0.83647\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 167 | loss: 0.83647 - acc: 0.8885 -- iter: 24/24\n",
            "--\n",
            "Training Step: 168  | total loss: \u001b[1m\u001b[32m1.11777\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 168 | loss: 1.11777 - acc: 0.8205 -- iter: 24/24\n",
            "--\n",
            "Training Step: 169  | total loss: \u001b[1m\u001b[32m1.07683\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 169 | loss: 1.07683 - acc: 0.8301 -- iter: 24/24\n",
            "--\n",
            "Training Step: 170  | total loss: \u001b[1m\u001b[32m1.03957\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 170 | loss: 1.03957 - acc: 0.8429 -- iter: 24/24\n",
            "--\n",
            "Training Step: 171  | total loss: \u001b[1m\u001b[32m1.00558\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 171 | loss: 1.00558 - acc: 0.8545 -- iter: 24/24\n",
            "--\n",
            "Training Step: 172  | total loss: \u001b[1m\u001b[32m1.32907\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 172 | loss: 1.32907 - acc: 0.7774 -- iter: 24/24\n",
            "--\n",
            "Training Step: 173  | total loss: \u001b[1m\u001b[32m1.26551\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 173 | loss: 1.26551 - acc: 0.7955 -- iter: 24/24\n",
            "--\n",
            "Training Step: 174  | total loss: \u001b[1m\u001b[32m1.20812\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 174 | loss: 1.20812 - acc: 0.8117 -- iter: 24/24\n",
            "--\n",
            "Training Step: 175  | total loss: \u001b[1m\u001b[32m1.15622\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 175 | loss: 1.15622 - acc: 0.8264 -- iter: 24/24\n",
            "--\n",
            "Training Step: 176  | total loss: \u001b[1m\u001b[32m1.10922\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 176 | loss: 1.10922 - acc: 0.8396 -- iter: 24/24\n",
            "--\n",
            "Training Step: 177  | total loss: \u001b[1m\u001b[32m1.06658\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 177 | loss: 1.06658 - acc: 0.8515 -- iter: 24/24\n",
            "--\n",
            "Training Step: 178  | total loss: \u001b[1m\u001b[32m1.02783\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 178 | loss: 1.02783 - acc: 0.8663 -- iter: 24/24\n",
            "--\n",
            "Training Step: 179  | total loss: \u001b[1m\u001b[32m0.99253\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 179 | loss: 0.99253 - acc: 0.8797 -- iter: 24/24\n",
            "--\n",
            "Training Step: 180  | total loss: \u001b[1m\u001b[32m0.96031\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 180 | loss: 0.96031 - acc: 0.8917 -- iter: 24/24\n",
            "--\n",
            "Training Step: 181  | total loss: \u001b[1m\u001b[32m0.93082\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 181 | loss: 0.93082 - acc: 0.9025 -- iter: 24/24\n",
            "--\n",
            "Training Step: 182  | total loss: \u001b[1m\u001b[32m0.90377\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 182 | loss: 0.90377 - acc: 0.9123 -- iter: 24/24\n",
            "--\n",
            "Training Step: 183  | total loss: \u001b[1m\u001b[32m0.87888\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 183 | loss: 0.87888 - acc: 0.9211 -- iter: 24/24\n",
            "--\n",
            "Training Step: 184  | total loss: \u001b[1m\u001b[32m1.15622\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 184 | loss: 1.15622 - acc: 0.8415 -- iter: 24/24\n",
            "--\n",
            "Training Step: 185  | total loss: \u001b[1m\u001b[32m1.10526\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 185 | loss: 1.10526 - acc: 0.8573 -- iter: 24/24\n",
            "--\n",
            "Training Step: 186  | total loss: \u001b[1m\u001b[32m1.39151\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 186 | loss: 1.39151 - acc: 0.7799 -- iter: 24/24\n",
            "--\n",
            "Training Step: 187  | total loss: \u001b[1m\u001b[32m1.31670\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 187 | loss: 1.31670 - acc: 0.8019 -- iter: 24/24\n",
            "--\n",
            "Training Step: 188  | total loss: \u001b[1m\u001b[32m1.24929\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 188 | loss: 1.24929 - acc: 0.8217 -- iter: 24/24\n",
            "--\n",
            "Training Step: 189  | total loss: \u001b[1m\u001b[32m1.18849\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 189 | loss: 1.18849 - acc: 0.8396 -- iter: 24/24\n",
            "--\n",
            "Training Step: 190  | total loss: \u001b[1m\u001b[32m1.13356\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 190 | loss: 1.13356 - acc: 0.8556 -- iter: 24/24\n",
            "--\n",
            "Training Step: 191  | total loss: \u001b[1m\u001b[32m1.08388\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 191 | loss: 1.08388 - acc: 0.8700 -- iter: 24/24\n",
            "--\n",
            "Training Step: 192  | total loss: \u001b[1m\u001b[32m1.03886\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 192 | loss: 1.03886 - acc: 0.8830 -- iter: 24/24\n",
            "--\n",
            "Training Step: 193  | total loss: \u001b[1m\u001b[32m0.99799\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 193 | loss: 0.99799 - acc: 0.8947 -- iter: 24/24\n",
            "--\n",
            "Training Step: 194  | total loss: \u001b[1m\u001b[32m0.96082\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 194 | loss: 0.96082 - acc: 0.9053 -- iter: 24/24\n",
            "--\n",
            "Training Step: 195  | total loss: \u001b[1m\u001b[32m0.92693\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 195 | loss: 0.92693 - acc: 0.9147 -- iter: 24/24\n",
            "--\n",
            "Training Step: 196  | total loss: \u001b[1m\u001b[32m0.89598\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 196 | loss: 0.89598 - acc: 0.9233 -- iter: 24/24\n",
            "--\n",
            "Training Step: 197  | total loss: \u001b[1m\u001b[32m0.86763\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 197 | loss: 0.86763 - acc: 0.9309 -- iter: 24/24\n",
            "--\n",
            "Training Step: 198  | total loss: \u001b[1m\u001b[32m0.84159\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 198 | loss: 0.84159 - acc: 0.9378 -- iter: 24/24\n",
            "--\n",
            "Training Step: 199  | total loss: \u001b[1m\u001b[32m0.81762\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 199 | loss: 0.81762 - acc: 0.9441 -- iter: 24/24\n",
            "--\n",
            "Training Step: 200  | total loss: \u001b[1m\u001b[32m0.77498\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 200 | loss: 0.77498 - acc: 0.9497 -- iter: 24/24\n",
            "--\n",
            "Training Step: 201  | total loss: \u001b[1m\u001b[32m0.77498\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 201 | loss: 0.77498 - acc: 0.9547 -- iter: 24/24\n",
            "--\n",
            "Training Step: 202  | total loss: \u001b[1m\u001b[32m0.75593\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 202 | loss: 0.75593 - acc: 0.9592 -- iter: 24/24\n",
            "--\n",
            "Training Step: 203  | total loss: \u001b[1m\u001b[32m0.73818\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 203 | loss: 0.73818 - acc: 0.9633 -- iter: 24/24\n",
            "--\n",
            "Training Step: 204  | total loss: \u001b[1m\u001b[32m1.03576\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 204 | loss: 1.03576 - acc: 0.8795 -- iter: 24/24\n",
            "--\n",
            "Training Step: 205  | total loss: \u001b[1m\u001b[32m0.98909\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 205 | loss: 0.98909 - acc: 0.8915 -- iter: 24/24\n",
            "--\n",
            "Training Step: 206  | total loss: \u001b[1m\u001b[32m0.94673\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 206 | loss: 0.94673 - acc: 0.9024 -- iter: 24/24\n",
            "--\n",
            "Training Step: 207  | total loss: \u001b[1m\u001b[32m0.90823\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 207 | loss: 0.90823 - acc: 0.9121 -- iter: 24/24\n",
            "--\n",
            "Training Step: 208  | total loss: \u001b[1m\u001b[32m0.87315\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 208 | loss: 0.87315 - acc: 0.9209 -- iter: 24/24\n",
            "--\n",
            "Training Step: 209  | total loss: \u001b[1m\u001b[32m0.84114\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 209 | loss: 0.84114 - acc: 0.9288 -- iter: 24/24\n",
            "--\n",
            "Training Step: 210  | total loss: \u001b[1m\u001b[32m0.81185\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 210 | loss: 0.81185 - acc: 0.9423 -- iter: 24/24\n",
            "--\n",
            "Training Step: 211  | total loss: \u001b[1m\u001b[32m0.78500\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 211 | loss: 0.78500 - acc: 0.9423 -- iter: 24/24\n",
            "--\n",
            "Training Step: 212  | total loss: \u001b[1m\u001b[32m0.76032\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 212 | loss: 0.76032 - acc: 0.9481 -- iter: 24/24\n",
            "--\n",
            "Training Step: 213  | total loss: \u001b[1m\u001b[32m0.73758\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 213 | loss: 0.73758 - acc: 0.9533 -- iter: 24/24\n",
            "--\n",
            "Training Step: 214  | total loss: \u001b[1m\u001b[32m0.71656\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 214 | loss: 0.71656 - acc: 0.9580 -- iter: 24/24\n",
            "--\n",
            "Training Step: 215  | total loss: \u001b[1m\u001b[32m0.69708\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 215 | loss: 0.69708 - acc: 0.9622 -- iter: 24/24\n",
            "--\n",
            "Training Step: 216  | total loss: \u001b[1m\u001b[32m1.05555\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 216 | loss: 1.05555 - acc: 0.8743 -- iter: 24/24\n",
            "--\n",
            "Training Step: 217  | total loss: \u001b[1m\u001b[32m0.95232\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 217 | loss: 0.95232 - acc: 0.8869 -- iter: 24/24\n",
            "--\n",
            "Training Step: 218  | total loss: \u001b[1m\u001b[32m0.95232\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 218 | loss: 0.95232 - acc: 0.8982 -- iter: 24/24\n",
            "--\n",
            "Training Step: 219  | total loss: \u001b[1m\u001b[32m0.90787\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 219 | loss: 0.90787 - acc: 0.9084 -- iter: 24/24\n",
            "--\n",
            "Training Step: 220  | total loss: \u001b[1m\u001b[32m0.86752\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 220 | loss: 0.86752 - acc: 0.9175 -- iter: 24/24\n",
            "--\n",
            "Training Step: 221  | total loss: \u001b[1m\u001b[32m0.83084\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 221 | loss: 0.83084 - acc: 0.9258 -- iter: 24/24\n",
            "--\n",
            "Training Step: 222  | total loss: \u001b[1m\u001b[32m0.79742\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 222 | loss: 0.79742 - acc: 0.9332 -- iter: 24/24\n",
            "--\n",
            "Training Step: 223  | total loss: \u001b[1m\u001b[32m0.76691\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 223 | loss: 0.76691 - acc: 0.9399 -- iter: 24/24\n",
            "--\n",
            "Training Step: 224  | total loss: \u001b[1m\u001b[32m1.10701\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 224 | loss: 1.10701 - acc: 0.8584 -- iter: 24/24\n",
            "--\n",
            "Training Step: 225  | total loss: \u001b[1m\u001b[32m1.04497\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 225 | loss: 1.04497 - acc: 0.8725 -- iter: 24/24\n",
            "--\n",
            "Training Step: 226  | total loss: \u001b[1m\u001b[32m0.98896\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 226 | loss: 0.98896 - acc: 0.8853 -- iter: 24/24\n",
            "--\n",
            "Training Step: 227  | total loss: \u001b[1m\u001b[32m0.93833\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 227 | loss: 0.93833 - acc: 0.8968 -- iter: 24/24\n",
            "--\n",
            "Training Step: 228  | total loss: \u001b[1m\u001b[32m1.17644\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 228 | loss: 1.17644 - acc: 0.8196 -- iter: 24/24\n",
            "--\n",
            "Training Step: 229  | total loss: \u001b[1m\u001b[32m1.10675\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 229 | loss: 1.10675 - acc: 0.8376 -- iter: 24/24\n",
            "--\n",
            "Training Step: 230  | total loss: \u001b[1m\u001b[32m1.04394\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 230 | loss: 1.04394 - acc: 0.8539 -- iter: 24/24\n",
            "--\n",
            "Training Step: 231  | total loss: \u001b[1m\u001b[32m0.98726\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 231 | loss: 0.98726 - acc: 0.8685 -- iter: 24/24\n",
            "--\n",
            "Training Step: 232  | total loss: \u001b[1m\u001b[32m0.93604\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 232 | loss: 0.93604 - acc: 0.8816 -- iter: 24/24\n",
            "--\n",
            "Training Step: 233  | total loss: \u001b[1m\u001b[32m0.88970\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 233 | loss: 0.88970 - acc: 0.8935 -- iter: 24/24\n",
            "--\n",
            "Training Step: 234  | total loss: \u001b[1m\u001b[32m0.84771\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 234 | loss: 0.84771 - acc: 0.9041 -- iter: 24/24\n",
            "--\n",
            "Training Step: 235  | total loss: \u001b[1m\u001b[32m0.80960\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 235 | loss: 0.80960 - acc: 0.9137 -- iter: 24/24\n",
            "--\n",
            "Training Step: 236  | total loss: \u001b[1m\u001b[32m0.77495\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 236 | loss: 0.77495 - acc: 0.9223 -- iter: 24/24\n",
            "--\n",
            "Training Step: 237  | total loss: \u001b[1m\u001b[32m0.74339\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 237 | loss: 0.74339 - acc: 0.9301 -- iter: 24/24\n",
            "--\n",
            "Training Step: 238  | total loss: \u001b[1m\u001b[32m0.71458\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 238 | loss: 0.71458 - acc: 0.9371 -- iter: 24/24\n",
            "--\n",
            "Training Step: 239  | total loss: \u001b[1m\u001b[32m0.68823\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 239 | loss: 0.68823 - acc: 0.9434 -- iter: 24/24\n",
            "--\n",
            "Training Step: 240  | total loss: \u001b[1m\u001b[32m0.66406\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 240 | loss: 0.66406 - acc: 0.9490 -- iter: 24/24\n",
            "--\n",
            "Training Step: 241  | total loss: \u001b[1m\u001b[32m0.64185\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 241 | loss: 0.64185 - acc: 0.9541 -- iter: 24/24\n",
            "--\n",
            "Training Step: 242  | total loss: \u001b[1m\u001b[32m1.03015\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 242 | loss: 1.03015 - acc: 0.8629 -- iter: 24/24\n",
            "--\n",
            "Training Step: 243  | total loss: \u001b[1m\u001b[32m0.97072\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 243 | loss: 0.97072 - acc: 0.8766 -- iter: 24/24\n",
            "--\n",
            "Training Step: 244  | total loss: \u001b[1m\u001b[32m1.19482\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 244 | loss: 1.19482 - acc: 0.8139 -- iter: 24/24\n",
            "--\n",
            "Training Step: 245  | total loss: \u001b[1m\u001b[32m1.11877\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 245 | loss: 1.11877 - acc: 0.8325 -- iter: 24/24\n",
            "--\n",
            "Training Step: 246  | total loss: \u001b[1m\u001b[32m1.50533\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 246 | loss: 1.50533 - acc: 0.7535 -- iter: 24/24\n",
            "--\n",
            "Training Step: 247  | total loss: \u001b[1m\u001b[32m1.39848\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 247 | loss: 1.39848 - acc: 0.7781 -- iter: 24/24\n",
            "--\n",
            "Training Step: 248  | total loss: \u001b[1m\u001b[32m1.30252\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 248 | loss: 1.30252 - acc: 0.8003 -- iter: 24/24\n",
            "--\n",
            "Training Step: 249  | total loss: \u001b[1m\u001b[32m1.21630\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 249 | loss: 1.21630 - acc: 0.8203 -- iter: 24/24\n",
            "--\n",
            "Training Step: 250  | total loss: \u001b[1m\u001b[32m1.13876\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 250 | loss: 1.13876 - acc: 0.8382 -- iter: 24/24\n",
            "--\n",
            "Training Step: 251  | total loss: \u001b[1m\u001b[32m1.06898\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 251 | loss: 1.06898 - acc: 0.8544 -- iter: 24/24\n",
            "--\n",
            "Training Step: 252  | total loss: \u001b[1m\u001b[32m1.00611\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 252 | loss: 1.00611 - acc: 0.8690 -- iter: 24/24\n",
            "--\n",
            "Training Step: 253  | total loss: \u001b[1m\u001b[32m0.94942\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 253 | loss: 0.94942 - acc: 0.8821 -- iter: 24/24\n",
            "--\n",
            "Training Step: 254  | total loss: \u001b[1m\u001b[32m0.85196\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 254 | loss: 0.85196 - acc: 0.9045 -- iter: 24/24\n",
            "--\n",
            "Training Step: 255  | total loss: \u001b[1m\u001b[32m0.85196\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 255 | loss: 0.85196 - acc: 0.9045 -- iter: 24/24\n",
            "--\n",
            "Training Step: 256  | total loss: \u001b[1m\u001b[32m1.16672\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 256 | loss: 1.16672 - acc: 0.8182 -- iter: 24/24\n",
            "--\n",
            "Training Step: 257  | total loss: \u001b[1m\u001b[32m1.09336\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 257 | loss: 1.09336 - acc: 0.8364 -- iter: 24/24\n",
            "--\n",
            "Training Step: 258  | total loss: \u001b[1m\u001b[32m1.02731\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 258 | loss: 1.02731 - acc: 0.8527 -- iter: 24/24\n",
            "--\n",
            "Training Step: 259  | total loss: \u001b[1m\u001b[32m0.96776\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 259 | loss: 0.96776 - acc: 0.8675 -- iter: 24/24\n",
            "--\n",
            "Training Step: 260  | total loss: \u001b[1m\u001b[32m0.91402\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 260 | loss: 0.91402 - acc: 0.8807 -- iter: 24/24\n",
            "--\n",
            "Training Step: 261  | total loss: \u001b[1m\u001b[32m0.86547\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 261 | loss: 0.86547 - acc: 0.8927 -- iter: 24/24\n",
            "--\n",
            "Training Step: 262  | total loss: \u001b[1m\u001b[32m0.82154\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 262 | loss: 0.82154 - acc: 0.9034 -- iter: 24/24\n",
            "--\n",
            "Training Step: 263  | total loss: \u001b[1m\u001b[32m0.78174\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 263 | loss: 0.78174 - acc: 0.9130 -- iter: 24/24\n",
            "--\n",
            "Training Step: 264  | total loss: \u001b[1m\u001b[32m0.74562\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 264 | loss: 0.74562 - acc: 0.9217 -- iter: 24/24\n",
            "--\n",
            "Training Step: 265  | total loss: \u001b[1m\u001b[32m0.71279\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 265 | loss: 0.71279 - acc: 0.9296 -- iter: 24/24\n",
            "--\n",
            "Training Step: 266  | total loss: \u001b[1m\u001b[32m0.68289\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 266 | loss: 0.68289 - acc: 0.9366 -- iter: 24/24\n",
            "--\n",
            "Training Step: 267  | total loss: \u001b[1m\u001b[32m0.65561\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 267 | loss: 0.65561 - acc: 0.9430 -- iter: 24/24\n",
            "--\n",
            "Training Step: 268  | total loss: \u001b[1m\u001b[32m0.63067\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 268 | loss: 0.63067 - acc: 0.9487 -- iter: 24/24\n",
            "--\n",
            "Training Step: 269  | total loss: \u001b[1m\u001b[32m0.60781\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 269 | loss: 0.60781 - acc: 0.9538 -- iter: 24/24\n",
            "--\n",
            "Training Step: 270  | total loss: \u001b[1m\u001b[32m0.93967\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 270 | loss: 0.93967 - acc: 0.8667 -- iter: 24/24\n",
            "--\n",
            "Training Step: 271  | total loss: \u001b[1m\u001b[32m0.88537\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 271 | loss: 0.88537 - acc: 0.8801 -- iter: 24/24\n",
            "--\n",
            "Training Step: 272  | total loss: \u001b[1m\u001b[32m0.83633\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 272 | loss: 0.83633 - acc: 0.8921 -- iter: 24/24\n",
            "--\n",
            "Training Step: 273  | total loss: \u001b[1m\u001b[32m0.79200\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 273 | loss: 0.79200 - acc: 0.9029 -- iter: 24/24\n",
            "--\n",
            "Training Step: 274  | total loss: \u001b[1m\u001b[32m1.06023\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 274 | loss: 1.06023 - acc: 0.8292 -- iter: 24/24\n",
            "--\n",
            "Training Step: 275  | total loss: \u001b[1m\u001b[32m0.99326\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 275 | loss: 0.99326 - acc: 0.8463 -- iter: 24/24\n",
            "--\n",
            "Training Step: 276  | total loss: \u001b[1m\u001b[32m0.93292\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 276 | loss: 0.93292 - acc: 0.8617 -- iter: 24/24\n",
            "--\n",
            "Training Step: 277  | total loss: \u001b[1m\u001b[32m0.87851\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 277 | loss: 0.87851 - acc: 0.8755 -- iter: 24/24\n",
            "--\n",
            "Training Step: 278  | total loss: \u001b[1m\u001b[32m1.17011\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 278 | loss: 1.17011 - acc: 0.8046 -- iter: 24/24\n",
            "--\n",
            "Training Step: 279  | total loss: \u001b[1m\u001b[32m1.09192\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 279 | loss: 1.09192 - acc: 0.8242 -- iter: 24/24\n",
            "--\n",
            "Training Step: 280  | total loss: \u001b[1m\u001b[32m1.42325\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 280 | loss: 1.42325 - acc: 0.7501 -- iter: 24/24\n",
            "--\n",
            "Training Step: 281  | total loss: \u001b[1m\u001b[32m1.22738\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 281 | loss: 1.22738 - acc: 0.7976 -- iter: 24/24\n",
            "--\n",
            "Training Step: 282  | total loss: \u001b[1m\u001b[32m1.14412\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 282 | loss: 1.14412 - acc: 0.8178 -- iter: 24/24\n",
            "--\n",
            "Training Step: 283  | total loss: \u001b[1m\u001b[32m1.14412\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 283 | loss: 1.14412 - acc: 0.8178 -- iter: 24/24\n",
            "--\n",
            "Training Step: 284  | total loss: \u001b[1m\u001b[32m1.06926\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 284 | loss: 1.06926 - acc: 0.8360 -- iter: 24/24\n",
            "--\n",
            "Training Step: 285  | total loss: \u001b[1m\u001b[32m1.00190\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 285 | loss: 1.00190 - acc: 0.8524 -- iter: 24/24\n",
            "--\n",
            "Training Step: 286  | total loss: \u001b[1m\u001b[32m0.94124\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 286 | loss: 0.94124 - acc: 0.8672 -- iter: 24/24\n",
            "--\n",
            "Training Step: 287  | total loss: \u001b[1m\u001b[32m0.88656\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 287 | loss: 0.88656 - acc: 0.8805 -- iter: 24/24\n",
            "--\n",
            "Training Step: 288  | total loss: \u001b[1m\u001b[32m0.83721\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 288 | loss: 0.83721 - acc: 0.8924 -- iter: 24/24\n",
            "--\n",
            "Training Step: 289  | total loss: \u001b[1m\u001b[32m0.79263\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 289 | loss: 0.79263 - acc: 0.9032 -- iter: 24/24\n",
            "--\n",
            "Training Step: 290  | total loss: \u001b[1m\u001b[32m0.75229\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 290 | loss: 0.75229 - acc: 0.9129 -- iter: 24/24\n",
            "--\n",
            "Training Step: 291  | total loss: \u001b[1m\u001b[32m0.71574\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 291 | loss: 0.71574 - acc: 0.9216 -- iter: 24/24\n",
            "--\n",
            "Training Step: 292  | total loss: \u001b[1m\u001b[32m0.99772\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 292 | loss: 0.99772 - acc: 0.8419 -- iter: 24/24\n",
            "--\n",
            "Training Step: 293  | total loss: \u001b[1m\u001b[32m0.93632\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 293 | loss: 0.93632 - acc: 0.8577 -- iter: 24/24\n",
            "--\n",
            "Training Step: 294  | total loss: \u001b[1m\u001b[32m1.22427\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 294 | loss: 1.22427 - acc: 0.7928 -- iter: 24/24\n",
            "--\n",
            "Training Step: 295  | total loss: \u001b[1m\u001b[32m1.14027\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 295 | loss: 1.14027 - acc: 0.8135 -- iter: 24/24\n",
            "--\n",
            "Training Step: 296  | total loss: \u001b[1m\u001b[32m1.47663\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 296 | loss: 1.47663 - acc: 0.7405 -- iter: 24/24\n",
            "--\n",
            "Training Step: 297  | total loss: \u001b[1m\u001b[32m1.36781\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 297 | loss: 1.36781 - acc: 0.7664 -- iter: 24/24\n",
            "--\n",
            "Training Step: 298  | total loss: \u001b[1m\u001b[32m1.64617\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 298 | loss: 1.64617 - acc: 0.7065 -- iter: 24/24\n",
            "--\n",
            "Training Step: 299  | total loss: \u001b[1m\u001b[32m1.52115\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 299 | loss: 1.52115 - acc: 0.7358 -- iter: 24/24\n",
            "--\n",
            "Training Step: 300  | total loss: \u001b[1m\u001b[32m1.40902\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 300 | loss: 1.40902 - acc: 0.7622 -- iter: 24/24\n",
            "--\n",
            "Training Step: 301  | total loss: \u001b[1m\u001b[32m1.30841\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 301 | loss: 1.30841 - acc: 0.7860 -- iter: 24/24\n",
            "--\n",
            "Training Step: 302  | total loss: \u001b[1m\u001b[32m1.50331\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 302 | loss: 1.50331 - acc: 0.7241 -- iter: 24/24\n",
            "--\n",
            "Training Step: 303  | total loss: \u001b[1m\u001b[32m1.39387\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 303 | loss: 1.39387 - acc: 0.7517 -- iter: 24/24\n",
            "--\n",
            "Training Step: 304  | total loss: \u001b[1m\u001b[32m1.29567\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 304 | loss: 1.29567 - acc: 0.7765 -- iter: 24/24\n",
            "--\n",
            "Training Step: 305  | total loss: \u001b[1m\u001b[32m1.20749\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 305 | loss: 1.20749 - acc: 0.7989 -- iter: 24/24\n",
            "--\n",
            "Training Step: 306  | total loss: \u001b[1m\u001b[32m1.51504\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 306 | loss: 1.51504 - acc: 0.7231 -- iter: 24/24\n",
            "--\n",
            "Training Step: 307  | total loss: \u001b[1m\u001b[32m1.40544\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 307 | loss: 1.40544 - acc: 0.7508 -- iter: 24/24\n",
            "--\n",
            "Training Step: 308  | total loss: \u001b[1m\u001b[32m1.30709\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 308 | loss: 1.30709 - acc: 0.7757 -- iter: 24/24\n",
            "--\n",
            "Training Step: 309  | total loss: \u001b[1m\u001b[32m1.21878\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 309 | loss: 1.21878 - acc: 0.7982 -- iter: 24/24\n",
            "--\n",
            "Training Step: 310  | total loss: \u001b[1m\u001b[32m1.13944\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 310 | loss: 1.13944 - acc: 0.8183 -- iter: 24/24\n",
            "--\n",
            "Training Step: 311  | total loss: \u001b[1m\u001b[32m1.06808\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 311 | loss: 1.06808 - acc: 0.8365 -- iter: 24/24\n",
            "--\n",
            "Training Step: 312  | total loss: \u001b[1m\u001b[32m1.33638\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 312 | loss: 1.33638 - acc: 0.7695 -- iter: 24/24\n",
            "--\n",
            "Training Step: 313  | total loss: \u001b[1m\u001b[32m1.24556\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 313 | loss: 1.24556 - acc: 0.7926 -- iter: 24/24\n",
            "--\n",
            "Training Step: 314  | total loss: \u001b[1m\u001b[32m1.42265\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 314 | loss: 1.42265 - acc: 0.7258 -- iter: 24/24\n",
            "--\n",
            "Training Step: 315  | total loss: \u001b[1m\u001b[32m1.32367\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 315 | loss: 1.32367 - acc: 0.7532 -- iter: 24/24\n",
            "--\n",
            "Training Step: 316  | total loss: \u001b[1m\u001b[32m1.23481\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 316 | loss: 1.23481 - acc: 0.7779 -- iter: 24/24\n",
            "--\n",
            "Training Step: 317  | total loss: \u001b[1m\u001b[32m1.15498\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 317 | loss: 1.15498 - acc: 0.8001 -- iter: 24/24\n",
            "--\n",
            "Training Step: 318  | total loss: \u001b[1m\u001b[32m1.08319\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 318 | loss: 1.08319 - acc: 0.8381 -- iter: 24/24\n",
            "--\n",
            "Training Step: 319  | total loss: \u001b[1m\u001b[32m1.01856\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 319 | loss: 1.01856 - acc: 0.8381 -- iter: 24/24\n",
            "--\n",
            "Training Step: 320  | total loss: \u001b[1m\u001b[32m1.20376\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 320 | loss: 1.20376 - acc: 0.7793 -- iter: 24/24\n",
            "--\n",
            "Training Step: 321  | total loss: \u001b[1m\u001b[32m1.12708\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 321 | loss: 1.12708 - acc: 0.8014 -- iter: 24/24\n",
            "--\n",
            "Training Step: 322  | total loss: \u001b[1m\u001b[32m1.05808\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 322 | loss: 1.05808 - acc: 0.8212 -- iter: 24/24\n",
            "--\n",
            "Training Step: 323  | total loss: \u001b[1m\u001b[32m0.99592\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 323 | loss: 0.99592 - acc: 0.8391 -- iter: 24/24\n",
            "--\n",
            "Training Step: 324  | total loss: \u001b[1m\u001b[32m0.93985\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 324 | loss: 0.93985 - acc: 0.8552 -- iter: 24/24\n",
            "--\n",
            "Training Step: 325  | total loss: \u001b[1m\u001b[32m0.88922\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 325 | loss: 0.88922 - acc: 0.8697 -- iter: 24/24\n",
            "--\n",
            "Training Step: 326  | total loss: \u001b[1m\u001b[32m0.84343\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 326 | loss: 0.84343 - acc: 0.8827 -- iter: 24/24\n",
            "--\n",
            "Training Step: 327  | total loss: \u001b[1m\u001b[32m0.80195\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 327 | loss: 0.80195 - acc: 0.8944 -- iter: 24/24\n",
            "--\n",
            "Training Step: 328  | total loss: \u001b[1m\u001b[32m0.76433\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 328 | loss: 0.76433 - acc: 0.9050 -- iter: 24/24\n",
            "--\n",
            "Training Step: 329  | total loss: \u001b[1m\u001b[32m0.73013\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 329 | loss: 0.73013 - acc: 0.9145 -- iter: 24/24\n",
            "--\n",
            "Training Step: 330  | total loss: \u001b[1m\u001b[32m0.69900\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 330 | loss: 0.69900 - acc: 0.9230 -- iter: 24/24\n",
            "--\n",
            "Training Step: 331  | total loss: \u001b[1m\u001b[32m0.67060\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 331 | loss: 0.67060 - acc: 0.9307 -- iter: 24/24\n",
            "--\n",
            "Training Step: 332  | total loss: \u001b[1m\u001b[32m0.64464\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 332 | loss: 0.64464 - acc: 0.9377 -- iter: 24/24\n",
            "--\n",
            "Training Step: 333  | total loss: \u001b[1m\u001b[32m0.62085\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 333 | loss: 0.62085 - acc: 0.9439 -- iter: 24/24\n",
            "--\n",
            "Training Step: 334  | total loss: \u001b[1m\u001b[32m0.59901\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 334 | loss: 0.59901 - acc: 0.9495 -- iter: 24/24\n",
            "--\n",
            "Training Step: 335  | total loss: \u001b[1m\u001b[32m0.57890\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 335 | loss: 0.57890 - acc: 0.9546 -- iter: 24/24\n",
            "--\n",
            "Training Step: 336  | total loss: \u001b[1m\u001b[32m0.56036\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 336 | loss: 0.56036 - acc: 0.9591 -- iter: 24/24\n",
            "--\n",
            "Training Step: 337  | total loss: \u001b[1m\u001b[32m0.54320\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 337 | loss: 0.54320 - acc: 0.9632 -- iter: 24/24\n",
            "--\n",
            "Training Step: 338  | total loss: \u001b[1m\u001b[32m0.71534\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 338 | loss: 0.71534 - acc: 0.9027 -- iter: 24/24\n",
            "--\n",
            "Training Step: 339  | total loss: \u001b[1m\u001b[32m0.68191\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 339 | loss: 0.68191 - acc: 0.9027 -- iter: 24/24\n",
            "--\n",
            "Training Step: 340  | total loss: \u001b[1m\u001b[32m0.65149\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 340 | loss: 0.65149 - acc: 0.9124 -- iter: 24/24\n",
            "--\n",
            "Training Step: 341  | total loss: \u001b[1m\u001b[32m0.62377\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 341 | loss: 0.62377 - acc: 0.9212 -- iter: 24/24\n",
            "--\n",
            "Training Step: 342  | total loss: \u001b[1m\u001b[32m0.91237\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 342 | loss: 0.91237 - acc: 0.8457 -- iter: 24/24\n",
            "--\n",
            "Training Step: 343  | total loss: \u001b[1m\u001b[32m0.85807\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 343 | loss: 0.85807 - acc: 0.8612 -- iter: 24/24\n",
            "--\n",
            "Training Step: 344  | total loss: \u001b[1m\u001b[32m0.80902\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 344 | loss: 0.80902 - acc: 0.8750 -- iter: 24/24\n",
            "--\n",
            "Training Step: 345  | total loss: \u001b[1m\u001b[32m0.76469\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 345 | loss: 0.76469 - acc: 0.8988 -- iter: 24/24\n",
            "--\n",
            "Training Step: 346  | total loss: \u001b[1m\u001b[32m0.72455\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 346 | loss: 0.72455 - acc: 0.8988 -- iter: 24/24\n",
            "--\n",
            "Training Step: 347  | total loss: \u001b[1m\u001b[32m0.68818\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 347 | loss: 0.68818 - acc: 0.9089 -- iter: 24/24\n",
            "--\n",
            "Training Step: 348  | total loss: \u001b[1m\u001b[32m0.65517\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 348 | loss: 0.65517 - acc: 0.9180 -- iter: 24/24\n",
            "--\n",
            "Training Step: 349  | total loss: \u001b[1m\u001b[32m0.62516\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 349 | loss: 0.62516 - acc: 0.9262 -- iter: 24/24\n",
            "--\n",
            "Training Step: 350  | total loss: \u001b[1m\u001b[32m0.59784\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 350 | loss: 0.59784 - acc: 0.9336 -- iter: 24/24\n",
            "--\n",
            "Training Step: 351  | total loss: \u001b[1m\u001b[32m0.57292\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 351 | loss: 0.57292 - acc: 0.9402 -- iter: 24/24\n",
            "--\n",
            "Training Step: 352  | total loss: \u001b[1m\u001b[32m0.55016\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 352 | loss: 0.55016 - acc: 0.9462 -- iter: 24/24\n",
            "--\n",
            "Training Step: 353  | total loss: \u001b[1m\u001b[32m0.52932\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 353 | loss: 0.52932 - acc: 0.9516 -- iter: 24/24\n",
            "--\n",
            "Training Step: 354  | total loss: \u001b[1m\u001b[32m0.51021\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 354 | loss: 0.51021 - acc: 0.9564 -- iter: 24/24\n",
            "--\n",
            "Training Step: 355  | total loss: \u001b[1m\u001b[32m0.49264\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 355 | loss: 0.49264 - acc: 0.9608 -- iter: 24/24\n",
            "--\n",
            "Training Step: 356  | total loss: \u001b[1m\u001b[32m0.47647\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 356 | loss: 0.47647 - acc: 0.9647 -- iter: 24/24\n",
            "--\n",
            "Training Step: 357  | total loss: \u001b[1m\u001b[32m0.46153\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 357 | loss: 0.46153 - acc: 0.9682 -- iter: 24/24\n",
            "--\n",
            "Training Step: 358  | total loss: \u001b[1m\u001b[32m0.78646\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 358 | loss: 0.78646 - acc: 0.8839 -- iter: 24/24\n",
            "--\n",
            "Training Step: 359  | total loss: \u001b[1m\u001b[32m0.74001\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 359 | loss: 0.74001 - acc: 0.8955 -- iter: 24/24\n",
            "--\n",
            "Training Step: 360  | total loss: \u001b[1m\u001b[32m1.06246\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 360 | loss: 1.06246 - acc: 0.8185 -- iter: 24/24\n",
            "--\n",
            "Training Step: 361  | total loss: \u001b[1m\u001b[32m0.98833\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 361 | loss: 0.98833 - acc: 0.8366 -- iter: 24/24\n",
            "--\n",
            "Training Step: 362  | total loss: \u001b[1m\u001b[32m0.92165\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 362 | loss: 0.92165 - acc: 0.8530 -- iter: 24/24\n",
            "--\n",
            "Training Step: 363  | total loss: \u001b[1m\u001b[32m0.86162\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 363 | loss: 0.86162 - acc: 0.8677 -- iter: 24/24\n",
            "--\n",
            "Training Step: 364  | total loss: \u001b[1m\u001b[32m0.80754\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 364 | loss: 0.80754 - acc: 0.8809 -- iter: 24/24\n",
            "--\n",
            "Training Step: 365  | total loss: \u001b[1m\u001b[32m0.75878\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 365 | loss: 0.75878 - acc: 0.8928 -- iter: 24/24\n",
            "--\n",
            "Training Step: 366  | total loss: \u001b[1m\u001b[32m1.03967\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 366 | loss: 1.03967 - acc: 0.8160 -- iter: 24/24\n",
            "--\n",
            "Training Step: 367  | total loss: \u001b[1m\u001b[32m0.96765\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 367 | loss: 0.96765 - acc: 0.8344 -- iter: 24/24\n",
            "--\n",
            "Training Step: 368  | total loss: \u001b[1m\u001b[32m0.90287\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 368 | loss: 0.90287 - acc: 0.8510 -- iter: 24/24\n",
            "--\n",
            "Training Step: 369  | total loss: \u001b[1m\u001b[32m0.84455\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 369 | loss: 0.84455 - acc: 0.8659 -- iter: 24/24\n",
            "--\n",
            "Training Step: 370  | total loss: \u001b[1m\u001b[32m0.79200\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 370 | loss: 0.79200 - acc: 0.8793 -- iter: 24/24\n",
            "--\n",
            "Training Step: 371  | total loss: \u001b[1m\u001b[32m0.74463\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 371 | loss: 0.74463 - acc: 0.8914 -- iter: 24/24\n",
            "--\n",
            "Training Step: 372  | total loss: \u001b[1m\u001b[32m0.70187\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 372 | loss: 0.70187 - acc: 0.9022 -- iter: 24/24\n",
            "--\n",
            "Training Step: 373  | total loss: \u001b[1m\u001b[32m0.66324\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 373 | loss: 0.66324 - acc: 0.9120 -- iter: 24/24\n",
            "--\n",
            "Training Step: 374  | total loss: \u001b[1m\u001b[32m0.62829\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 374 | loss: 0.62829 - acc: 0.9208 -- iter: 24/24\n",
            "--\n",
            "Training Step: 375  | total loss: \u001b[1m\u001b[32m0.59664\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 375 | loss: 0.59664 - acc: 0.9287 -- iter: 24/24\n",
            "--\n",
            "Training Step: 376  | total loss: \u001b[1m\u001b[32m0.90665\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 376 | loss: 0.90665 - acc: 0.8567 -- iter: 24/24\n",
            "--\n",
            "Training Step: 377  | total loss: \u001b[1m\u001b[32m0.84694\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 377 | loss: 0.84694 - acc: 0.8710 -- iter: 24/24\n",
            "--\n",
            "Training Step: 378  | total loss: \u001b[1m\u001b[32m1.02322\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 378 | loss: 1.02322 - acc: 0.8168 -- iter: 24/24\n",
            "--\n",
            "Training Step: 379  | total loss: \u001b[1m\u001b[32m1.02322\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 379 | loss: 1.02322 - acc: 0.8168 -- iter: 24/24\n",
            "--\n",
            "Training Step: 380  | total loss: \u001b[1m\u001b[32m0.95205\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 380 | loss: 0.95205 - acc: 0.8351 -- iter: 24/24\n",
            "--\n",
            "Training Step: 381  | total loss: \u001b[1m\u001b[32m0.88803\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 381 | loss: 0.88803 - acc: 0.8516 -- iter: 24/24\n",
            "--\n",
            "Training Step: 382  | total loss: \u001b[1m\u001b[32m0.83042\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 382 | loss: 0.83042 - acc: 0.8664 -- iter: 24/24\n",
            "--\n",
            "Training Step: 383  | total loss: \u001b[1m\u001b[32m0.77852\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 383 | loss: 0.77852 - acc: 0.8798 -- iter: 24/24\n",
            "--\n",
            "Training Step: 384  | total loss: \u001b[1m\u001b[32m0.73173\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 384 | loss: 0.73173 - acc: 0.8918 -- iter: 24/24\n",
            "--\n",
            "Training Step: 385  | total loss: \u001b[1m\u001b[32m0.68951\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 385 | loss: 0.68951 - acc: 0.9026 -- iter: 24/24\n",
            "--\n",
            "Training Step: 386  | total loss: \u001b[1m\u001b[32m1.00971\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 386 | loss: 1.00971 - acc: 0.8165 -- iter: 24/24\n",
            "--\n",
            "Training Step: 387  | total loss: \u001b[1m\u001b[32m0.93962\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 387 | loss: 0.93962 - acc: 0.8349 -- iter: 24/24\n",
            "--\n",
            "Training Step: 388  | total loss: \u001b[1m\u001b[32m0.87656\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 388 | loss: 0.87656 - acc: 0.8514 -- iter: 24/24\n",
            "--\n",
            "Training Step: 389  | total loss: \u001b[1m\u001b[32m0.81980\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 389 | loss: 0.81980 - acc: 0.8663 -- iter: 24/24\n",
            "--\n",
            "Training Step: 390  | total loss: \u001b[1m\u001b[32m0.76865\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 390 | loss: 0.76865 - acc: 0.8796 -- iter: 24/24\n",
            "--\n",
            "Training Step: 391  | total loss: \u001b[1m\u001b[32m0.72252\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 391 | loss: 0.72252 - acc: 0.8917 -- iter: 24/24\n",
            "--\n",
            "Training Step: 392  | total loss: \u001b[1m\u001b[32m0.68089\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 392 | loss: 0.68089 - acc: 0.9025 -- iter: 24/24\n",
            "--\n",
            "Training Step: 393  | total loss: \u001b[1m\u001b[32m0.64326\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 393 | loss: 0.64326 - acc: 0.9122 -- iter: 24/24\n",
            "--\n",
            "Training Step: 394  | total loss: \u001b[1m\u001b[32m0.60923\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 394 | loss: 0.60923 - acc: 0.9210 -- iter: 24/24\n",
            "--\n",
            "Training Step: 395  | total loss: \u001b[1m\u001b[32m0.57839\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 395 | loss: 0.57839 - acc: 0.9289 -- iter: 24/24\n",
            "--\n",
            "Training Step: 396  | total loss: \u001b[1m\u001b[32m0.55043\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 396 | loss: 0.55043 - acc: 0.9360 -- iter: 24/24\n",
            "--\n",
            "Training Step: 397  | total loss: \u001b[1m\u001b[32m0.52502\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 397 | loss: 0.52502 - acc: 0.9482 -- iter: 24/24\n",
            "--\n",
            "Training Step: 398  | total loss: \u001b[1m\u001b[32m0.50191\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 398 | loss: 0.50191 - acc: 0.9482 -- iter: 24/24\n",
            "--\n",
            "Training Step: 399  | total loss: \u001b[1m\u001b[32m0.48084\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 399 | loss: 0.48084 - acc: 0.9534 -- iter: 24/24\n",
            "--\n",
            "Training Step: 400  | total loss: \u001b[1m\u001b[32m0.46161\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 400 | loss: 0.46161 - acc: 0.9580 -- iter: 24/24\n",
            "--\n",
            "INFO:tensorflow:/content/modelo.tflearn is not in all_model_checkpoint_paths. Manually adding it.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqm4pZef7Cgw",
        "outputId": "5e2eae08-ec9a-460d-e3f4-75142accd137"
      },
      "source": [
        "def mainBot():\n",
        "  while True:\n",
        "    entrada = input(\"Tu: \")\n",
        "    num = [0 for _ in range(len(palabras))]\n",
        "    entradaProcesada = nltk.word_tokenize(entrada)\n",
        "    entradaProcesada = [stemmer.stem(palabra.lower()) for palabra in entradaProcesada]\n",
        "    for palabraIndividual in entradaProcesada:\n",
        "      #indice , elemento\n",
        "        for i,palabra in enumerate(palabras):\n",
        "            if palabra == palabraIndividual:\n",
        "                num[i] = 1 #se ocupa la palabra\n",
        "    resultados = modelo.predict([numpy.array(num)]) #probabilidad que que respuesta corresponde\n",
        "    resultadosIndices = numpy.argmax(resultados)\n",
        "    tag = tags[resultadosIndices]\n",
        "\n",
        "    for tagAux in datos[\"contenido\"]:\n",
        "      if  tagAux[\"tag\"] == tag:\n",
        "          respuesta = tagAux[\"respuestas\"]\n",
        "    #  else: \n",
        "    #      respuesta = [\"error\"]\n",
        "    #      exit()\n",
        "\n",
        "    print(\"BOT: \", random.choice(respuesta))\n",
        "    \n",
        "    #print(resultados)\n",
        "mainBot()    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tu: hola\n",
            "BOT:  Hola buenos dias, necesita ayuda\n",
            "Tu: cuando es el examen general?\n",
            "BOT:  el examen general sera el 28 de setiembre.\n",
            "Tu: cuando es el examen cepreuna?\n",
            "BOT:  el examen cepreuna sera el 2 de octubre. postulante\n",
            "Tu: como puedo inscribirme?\n",
            "BOT:  Para obtener mas informacion ingrese al siguiente link: https://admision.unap.edu.pe/admision\n",
            "Tu: adios\n",
            "BOT:  adios mi estimado(a)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}